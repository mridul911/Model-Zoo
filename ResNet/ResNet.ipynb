{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQlwEpSKr1v7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0DIg8iqzr-2I"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ppE0VCyKsAfP"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82,
     "referenced_widgets": [
      "1969bb5949b14a57b738edca76e7e491",
      "7a4405dc27a746f88d517895cceb4e2c",
      "e2aa9dbcaaa1451992b9bc4c03d7c0f3",
      "30956606c84a4aab97dc1e319aa0a95f",
      "fc2fe569dede45789a518eaa503ecc85",
      "a7055e1d40b843dda95121da9dc72cea",
      "0b27470a2ac345b185d697f3189ffe7f",
      "23832a2ff17346ee968ae64edef05432"
     ]
    },
    "colab_type": "code",
    "id": "kXFBXfDwsB8N",
    "outputId": "3f9ac952-e4b8-4c89-c1e9-9aa9e1014837"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1969bb5949b14a57b738edca76e7e491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data/\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data/',\n",
    "                                             train=True, \n",
    "                                             transform=transform,\n",
    "                                             download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data/',\n",
    "                                            train=False, \n",
    "                                            transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=100, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=100, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jiARWqJ0sEZB"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AlO27YZWsH0V"
   },
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VUB6yUcvsJmB"
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ic2cwPzEsLQ4"
   },
   "outputs": [],
   "source": [
    "model = ResNet(BasicBlock, [2, 2, 2, 2]).to(device)#here we can give whether implement basicblock or bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qt276_VysNJK"
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "num_epochs = 80\n",
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "abPNyoLKsO_X"
   },
   "outputs": [],
   "source": [
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Lxz-S4UUsRC1",
    "outputId": "43bc745e-9c31-4c8f-a8a4-aac18f0643ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Step [100/500] Loss: 1.4909\n",
      "Epoch [1/80], Step [200/500] Loss: 1.7373\n",
      "Epoch [1/80], Step [300/500] Loss: 1.3315\n",
      "Epoch [1/80], Step [400/500] Loss: 1.2161\n",
      "Epoch [1/80], Step [500/500] Loss: 1.2118\n",
      "Epoch [2/80], Step [100/500] Loss: 0.9498\n",
      "Epoch [2/80], Step [200/500] Loss: 0.9058\n",
      "Epoch [2/80], Step [300/500] Loss: 1.0373\n",
      "Epoch [2/80], Step [400/500] Loss: 0.8527\n",
      "Epoch [2/80], Step [500/500] Loss: 0.8817\n",
      "Epoch [3/80], Step [100/500] Loss: 0.8233\n",
      "Epoch [3/80], Step [200/500] Loss: 0.8272\n",
      "Epoch [3/80], Step [300/500] Loss: 0.7172\n",
      "Epoch [3/80], Step [400/500] Loss: 0.7266\n",
      "Epoch [3/80], Step [500/500] Loss: 0.6662\n",
      "Epoch [4/80], Step [100/500] Loss: 0.6436\n",
      "Epoch [4/80], Step [200/500] Loss: 0.5806\n",
      "Epoch [4/80], Step [300/500] Loss: 0.6699\n",
      "Epoch [4/80], Step [400/500] Loss: 0.6287\n",
      "Epoch [4/80], Step [500/500] Loss: 0.7519\n",
      "Epoch [5/80], Step [100/500] Loss: 0.6534\n",
      "Epoch [5/80], Step [200/500] Loss: 0.6046\n",
      "Epoch [5/80], Step [300/500] Loss: 0.5702\n",
      "Epoch [5/80], Step [400/500] Loss: 0.5487\n",
      "Epoch [5/80], Step [500/500] Loss: 0.5222\n",
      "Epoch [6/80], Step [100/500] Loss: 0.6700\n",
      "Epoch [6/80], Step [200/500] Loss: 0.4658\n",
      "Epoch [6/80], Step [300/500] Loss: 0.4940\n",
      "Epoch [6/80], Step [400/500] Loss: 0.5211\n",
      "Epoch [6/80], Step [500/500] Loss: 0.3343\n",
      "Epoch [7/80], Step [100/500] Loss: 0.4062\n",
      "Epoch [7/80], Step [200/500] Loss: 0.4797\n",
      "Epoch [7/80], Step [300/500] Loss: 0.2908\n",
      "Epoch [7/80], Step [400/500] Loss: 0.3784\n",
      "Epoch [7/80], Step [500/500] Loss: 0.2764\n",
      "Epoch [8/80], Step [100/500] Loss: 0.4575\n",
      "Epoch [8/80], Step [200/500] Loss: 0.3335\n",
      "Epoch [8/80], Step [300/500] Loss: 0.4669\n",
      "Epoch [8/80], Step [400/500] Loss: 0.3109\n",
      "Epoch [8/80], Step [500/500] Loss: 0.2795\n",
      "Epoch [9/80], Step [100/500] Loss: 0.3939\n",
      "Epoch [9/80], Step [200/500] Loss: 0.3454\n",
      "Epoch [9/80], Step [300/500] Loss: 0.3382\n",
      "Epoch [9/80], Step [400/500] Loss: 0.4587\n",
      "Epoch [9/80], Step [500/500] Loss: 0.3309\n",
      "Epoch [10/80], Step [100/500] Loss: 0.4350\n",
      "Epoch [10/80], Step [200/500] Loss: 0.4041\n",
      "Epoch [10/80], Step [300/500] Loss: 0.2749\n",
      "Epoch [10/80], Step [400/500] Loss: 0.1363\n",
      "Epoch [10/80], Step [500/500] Loss: 0.3238\n",
      "Epoch [11/80], Step [100/500] Loss: 0.2289\n",
      "Epoch [11/80], Step [200/500] Loss: 0.3218\n",
      "Epoch [11/80], Step [300/500] Loss: 0.2686\n",
      "Epoch [11/80], Step [400/500] Loss: 0.3824\n",
      "Epoch [11/80], Step [500/500] Loss: 0.3678\n",
      "Epoch [12/80], Step [100/500] Loss: 0.2275\n",
      "Epoch [12/80], Step [200/500] Loss: 0.1915\n",
      "Epoch [12/80], Step [300/500] Loss: 0.2682\n",
      "Epoch [12/80], Step [400/500] Loss: 0.2727\n",
      "Epoch [12/80], Step [500/500] Loss: 0.2250\n",
      "Epoch [13/80], Step [100/500] Loss: 0.2687\n",
      "Epoch [13/80], Step [200/500] Loss: 0.4505\n",
      "Epoch [13/80], Step [300/500] Loss: 0.2748\n",
      "Epoch [13/80], Step [400/500] Loss: 0.1725\n",
      "Epoch [13/80], Step [500/500] Loss: 0.3269\n",
      "Epoch [14/80], Step [100/500] Loss: 0.1803\n",
      "Epoch [14/80], Step [200/500] Loss: 0.2255\n",
      "Epoch [14/80], Step [300/500] Loss: 0.3457\n",
      "Epoch [14/80], Step [400/500] Loss: 0.1713\n",
      "Epoch [14/80], Step [500/500] Loss: 0.1839\n",
      "Epoch [15/80], Step [100/500] Loss: 0.1386\n",
      "Epoch [15/80], Step [200/500] Loss: 0.2308\n",
      "Epoch [15/80], Step [300/500] Loss: 0.2601\n",
      "Epoch [15/80], Step [400/500] Loss: 0.2543\n",
      "Epoch [15/80], Step [500/500] Loss: 0.0777\n",
      "Epoch [16/80], Step [100/500] Loss: 0.0975\n",
      "Epoch [16/80], Step [200/500] Loss: 0.2264\n",
      "Epoch [16/80], Step [300/500] Loss: 0.2377\n",
      "Epoch [16/80], Step [400/500] Loss: 0.3203\n",
      "Epoch [16/80], Step [500/500] Loss: 0.1684\n",
      "Epoch [17/80], Step [100/500] Loss: 0.1171\n",
      "Epoch [17/80], Step [200/500] Loss: 0.1134\n",
      "Epoch [17/80], Step [300/500] Loss: 0.1097\n",
      "Epoch [17/80], Step [400/500] Loss: 0.1655\n",
      "Epoch [17/80], Step [500/500] Loss: 0.1853\n",
      "Epoch [18/80], Step [100/500] Loss: 0.1899\n",
      "Epoch [18/80], Step [200/500] Loss: 0.3002\n",
      "Epoch [18/80], Step [300/500] Loss: 0.1639\n",
      "Epoch [18/80], Step [400/500] Loss: 0.1896\n",
      "Epoch [18/80], Step [500/500] Loss: 0.1610\n",
      "Epoch [19/80], Step [100/500] Loss: 0.0994\n",
      "Epoch [19/80], Step [200/500] Loss: 0.1253\n",
      "Epoch [19/80], Step [300/500] Loss: 0.1609\n",
      "Epoch [19/80], Step [400/500] Loss: 0.0487\n",
      "Epoch [19/80], Step [500/500] Loss: 0.1951\n",
      "Epoch [20/80], Step [100/500] Loss: 0.0866\n",
      "Epoch [20/80], Step [200/500] Loss: 0.0875\n",
      "Epoch [20/80], Step [300/500] Loss: 0.1314\n",
      "Epoch [20/80], Step [400/500] Loss: 0.0967\n",
      "Epoch [20/80], Step [500/500] Loss: 0.1076\n",
      "Epoch [21/80], Step [100/500] Loss: 0.1098\n",
      "Epoch [21/80], Step [200/500] Loss: 0.0640\n",
      "Epoch [21/80], Step [300/500] Loss: 0.0922\n",
      "Epoch [21/80], Step [400/500] Loss: 0.0737\n",
      "Epoch [21/80], Step [500/500] Loss: 0.0846\n",
      "Epoch [22/80], Step [100/500] Loss: 0.0334\n",
      "Epoch [22/80], Step [200/500] Loss: 0.0557\n",
      "Epoch [22/80], Step [300/500] Loss: 0.0433\n",
      "Epoch [22/80], Step [400/500] Loss: 0.0884\n",
      "Epoch [22/80], Step [500/500] Loss: 0.0540\n",
      "Epoch [23/80], Step [100/500] Loss: 0.1254\n",
      "Epoch [23/80], Step [200/500] Loss: 0.0880\n",
      "Epoch [23/80], Step [300/500] Loss: 0.0650\n",
      "Epoch [23/80], Step [400/500] Loss: 0.0798\n",
      "Epoch [23/80], Step [500/500] Loss: 0.0254\n",
      "Epoch [24/80], Step [100/500] Loss: 0.1022\n",
      "Epoch [24/80], Step [200/500] Loss: 0.1015\n",
      "Epoch [24/80], Step [300/500] Loss: 0.0635\n",
      "Epoch [24/80], Step [400/500] Loss: 0.0724\n",
      "Epoch [24/80], Step [500/500] Loss: 0.1277\n",
      "Epoch [25/80], Step [100/500] Loss: 0.0488\n",
      "Epoch [25/80], Step [200/500] Loss: 0.0279\n",
      "Epoch [25/80], Step [300/500] Loss: 0.0361\n",
      "Epoch [25/80], Step [400/500] Loss: 0.0841\n",
      "Epoch [25/80], Step [500/500] Loss: 0.0323\n",
      "Epoch [26/80], Step [100/500] Loss: 0.0832\n",
      "Epoch [26/80], Step [200/500] Loss: 0.0537\n",
      "Epoch [26/80], Step [300/500] Loss: 0.0241\n",
      "Epoch [26/80], Step [400/500] Loss: 0.0304\n",
      "Epoch [26/80], Step [500/500] Loss: 0.0542\n",
      "Epoch [27/80], Step [100/500] Loss: 0.0097\n",
      "Epoch [27/80], Step [200/500] Loss: 0.0216\n",
      "Epoch [27/80], Step [300/500] Loss: 0.0627\n",
      "Epoch [27/80], Step [400/500] Loss: 0.0503\n",
      "Epoch [27/80], Step [500/500] Loss: 0.0643\n",
      "Epoch [28/80], Step [100/500] Loss: 0.0203\n",
      "Epoch [28/80], Step [200/500] Loss: 0.0624\n",
      "Epoch [28/80], Step [300/500] Loss: 0.0449\n",
      "Epoch [28/80], Step [400/500] Loss: 0.0063\n",
      "Epoch [28/80], Step [500/500] Loss: 0.0994\n",
      "Epoch [29/80], Step [100/500] Loss: 0.0157\n",
      "Epoch [29/80], Step [200/500] Loss: 0.0484\n",
      "Epoch [29/80], Step [300/500] Loss: 0.0182\n",
      "Epoch [29/80], Step [400/500] Loss: 0.0260\n",
      "Epoch [29/80], Step [500/500] Loss: 0.0731\n",
      "Epoch [30/80], Step [100/500] Loss: 0.0340\n",
      "Epoch [30/80], Step [200/500] Loss: 0.0096\n",
      "Epoch [30/80], Step [300/500] Loss: 0.0376\n",
      "Epoch [30/80], Step [400/500] Loss: 0.0532\n",
      "Epoch [30/80], Step [500/500] Loss: 0.0353\n",
      "Epoch [31/80], Step [100/500] Loss: 0.0268\n",
      "Epoch [31/80], Step [200/500] Loss: 0.0393\n",
      "Epoch [31/80], Step [300/500] Loss: 0.0108\n",
      "Epoch [31/80], Step [400/500] Loss: 0.0396\n",
      "Epoch [31/80], Step [500/500] Loss: 0.0435\n",
      "Epoch [32/80], Step [100/500] Loss: 0.0135\n",
      "Epoch [32/80], Step [200/500] Loss: 0.0251\n",
      "Epoch [32/80], Step [300/500] Loss: 0.0285\n",
      "Epoch [32/80], Step [400/500] Loss: 0.0189\n",
      "Epoch [32/80], Step [500/500] Loss: 0.0623\n",
      "Epoch [33/80], Step [100/500] Loss: 0.0239\n",
      "Epoch [33/80], Step [200/500] Loss: 0.0344\n",
      "Epoch [33/80], Step [300/500] Loss: 0.0343\n",
      "Epoch [33/80], Step [400/500] Loss: 0.0401\n",
      "Epoch [33/80], Step [500/500] Loss: 0.0063\n",
      "Epoch [34/80], Step [100/500] Loss: 0.0029\n",
      "Epoch [34/80], Step [200/500] Loss: 0.0865\n",
      "Epoch [34/80], Step [300/500] Loss: 0.0077\n",
      "Epoch [34/80], Step [400/500] Loss: 0.0044\n",
      "Epoch [34/80], Step [500/500] Loss: 0.0243\n",
      "Epoch [35/80], Step [100/500] Loss: 0.0070\n",
      "Epoch [35/80], Step [200/500] Loss: 0.0252\n",
      "Epoch [35/80], Step [300/500] Loss: 0.0327\n",
      "Epoch [35/80], Step [400/500] Loss: 0.0185\n",
      "Epoch [35/80], Step [500/500] Loss: 0.0151\n",
      "Epoch [36/80], Step [100/500] Loss: 0.0145\n",
      "Epoch [36/80], Step [200/500] Loss: 0.0187\n",
      "Epoch [36/80], Step [300/500] Loss: 0.0189\n",
      "Epoch [36/80], Step [400/500] Loss: 0.0270\n",
      "Epoch [36/80], Step [500/500] Loss: 0.0244\n",
      "Epoch [37/80], Step [100/500] Loss: 0.0217\n",
      "Epoch [37/80], Step [200/500] Loss: 0.0122\n",
      "Epoch [37/80], Step [300/500] Loss: 0.0071\n",
      "Epoch [37/80], Step [400/500] Loss: 0.0208\n",
      "Epoch [37/80], Step [500/500] Loss: 0.0155\n",
      "Epoch [38/80], Step [100/500] Loss: 0.0268\n",
      "Epoch [38/80], Step [200/500] Loss: 0.0080\n",
      "Epoch [38/80], Step [300/500] Loss: 0.0607\n",
      "Epoch [38/80], Step [400/500] Loss: 0.0177\n",
      "Epoch [38/80], Step [500/500] Loss: 0.0041\n",
      "Epoch [39/80], Step [100/500] Loss: 0.0055\n",
      "Epoch [39/80], Step [200/500] Loss: 0.0073\n",
      "Epoch [39/80], Step [300/500] Loss: 0.0460\n",
      "Epoch [39/80], Step [400/500] Loss: 0.0125\n",
      "Epoch [39/80], Step [500/500] Loss: 0.0222\n",
      "Epoch [40/80], Step [100/500] Loss: 0.0388\n",
      "Epoch [40/80], Step [200/500] Loss: 0.0121\n",
      "Epoch [40/80], Step [300/500] Loss: 0.0045\n",
      "Epoch [40/80], Step [400/500] Loss: 0.0386\n",
      "Epoch [40/80], Step [500/500] Loss: 0.0715\n",
      "Epoch [41/80], Step [100/500] Loss: 0.0129\n",
      "Epoch [41/80], Step [200/500] Loss: 0.0090\n",
      "Epoch [41/80], Step [300/500] Loss: 0.0049\n",
      "Epoch [41/80], Step [400/500] Loss: 0.0053\n",
      "Epoch [41/80], Step [500/500] Loss: 0.0189\n",
      "Epoch [42/80], Step [100/500] Loss: 0.0021\n",
      "Epoch [42/80], Step [200/500] Loss: 0.0279\n",
      "Epoch [42/80], Step [300/500] Loss: 0.0024\n",
      "Epoch [42/80], Step [400/500] Loss: 0.0047\n",
      "Epoch [42/80], Step [500/500] Loss: 0.0025\n",
      "Epoch [43/80], Step [100/500] Loss: 0.0017\n",
      "Epoch [43/80], Step [200/500] Loss: 0.0007\n",
      "Epoch [43/80], Step [300/500] Loss: 0.0060\n",
      "Epoch [43/80], Step [400/500] Loss: 0.0292\n",
      "Epoch [43/80], Step [500/500] Loss: 0.0041\n",
      "Epoch [44/80], Step [100/500] Loss: 0.0028\n",
      "Epoch [44/80], Step [200/500] Loss: 0.0007\n",
      "Epoch [44/80], Step [300/500] Loss: 0.0125\n",
      "Epoch [44/80], Step [400/500] Loss: 0.0010\n",
      "Epoch [44/80], Step [500/500] Loss: 0.0088\n",
      "Epoch [45/80], Step [100/500] Loss: 0.0019\n",
      "Epoch [45/80], Step [200/500] Loss: 0.0084\n",
      "Epoch [45/80], Step [300/500] Loss: 0.0042\n",
      "Epoch [45/80], Step [400/500] Loss: 0.0061\n",
      "Epoch [45/80], Step [500/500] Loss: 0.0038\n",
      "Epoch [46/80], Step [100/500] Loss: 0.0044\n",
      "Epoch [46/80], Step [200/500] Loss: 0.0005\n",
      "Epoch [46/80], Step [300/500] Loss: 0.0003\n",
      "Epoch [46/80], Step [400/500] Loss: 0.0291\n",
      "Epoch [46/80], Step [500/500] Loss: 0.0348\n",
      "Epoch [47/80], Step [100/500] Loss: 0.0184\n",
      "Epoch [47/80], Step [200/500] Loss: 0.0040\n",
      "Epoch [47/80], Step [300/500] Loss: 0.0139\n",
      "Epoch [47/80], Step [400/500] Loss: 0.0029\n",
      "Epoch [47/80], Step [500/500] Loss: 0.0178\n",
      "Epoch [48/80], Step [100/500] Loss: 0.0049\n",
      "Epoch [48/80], Step [200/500] Loss: 0.0008\n",
      "Epoch [48/80], Step [300/500] Loss: 0.0007\n",
      "Epoch [48/80], Step [400/500] Loss: 0.0050\n",
      "Epoch [48/80], Step [500/500] Loss: 0.0009\n",
      "Epoch [49/80], Step [100/500] Loss: 0.0081\n",
      "Epoch [49/80], Step [200/500] Loss: 0.0043\n",
      "Epoch [49/80], Step [300/500] Loss: 0.0082\n",
      "Epoch [49/80], Step [400/500] Loss: 0.0037\n",
      "Epoch [49/80], Step [500/500] Loss: 0.0368\n",
      "Epoch [50/80], Step [100/500] Loss: 0.0090\n",
      "Epoch [50/80], Step [200/500] Loss: 0.0012\n",
      "Epoch [50/80], Step [300/500] Loss: 0.0015\n",
      "Epoch [50/80], Step [400/500] Loss: 0.0013\n",
      "Epoch [50/80], Step [500/500] Loss: 0.0164\n",
      "Epoch [51/80], Step [100/500] Loss: 0.0866\n",
      "Epoch [51/80], Step [200/500] Loss: 0.0032\n",
      "Epoch [51/80], Step [300/500] Loss: 0.0096\n",
      "Epoch [51/80], Step [400/500] Loss: 0.0037\n",
      "Epoch [51/80], Step [500/500] Loss: 0.0041\n",
      "Epoch [52/80], Step [100/500] Loss: 0.0107\n",
      "Epoch [52/80], Step [200/500] Loss: 0.0055\n",
      "Epoch [52/80], Step [300/500] Loss: 0.0005\n",
      "Epoch [52/80], Step [400/500] Loss: 0.0097\n",
      "Epoch [52/80], Step [500/500] Loss: 0.0162\n",
      "Epoch [53/80], Step [100/500] Loss: 0.0043\n",
      "Epoch [53/80], Step [200/500] Loss: 0.0009\n",
      "Epoch [53/80], Step [300/500] Loss: 0.0004\n",
      "Epoch [53/80], Step [400/500] Loss: 0.0341\n",
      "Epoch [53/80], Step [500/500] Loss: 0.0002\n",
      "Epoch [54/80], Step [100/500] Loss: 0.0025\n",
      "Epoch [54/80], Step [200/500] Loss: 0.0487\n",
      "Epoch [54/80], Step [300/500] Loss: 0.0003\n",
      "Epoch [54/80], Step [400/500] Loss: 0.0004\n",
      "Epoch [54/80], Step [500/500] Loss: 0.0007\n",
      "Epoch [55/80], Step [100/500] Loss: 0.0048\n",
      "Epoch [55/80], Step [200/500] Loss: 0.0497\n",
      "Epoch [55/80], Step [300/500] Loss: 0.0117\n",
      "Epoch [55/80], Step [400/500] Loss: 0.0034\n",
      "Epoch [55/80], Step [500/500] Loss: 0.0263\n",
      "Epoch [56/80], Step [100/500] Loss: 0.0012\n",
      "Epoch [56/80], Step [200/500] Loss: 0.0042\n",
      "Epoch [56/80], Step [300/500] Loss: 0.0003\n",
      "Epoch [56/80], Step [400/500] Loss: 0.0004\n",
      "Epoch [56/80], Step [500/500] Loss: 0.0006\n",
      "Epoch [57/80], Step [100/500] Loss: 0.0018\n",
      "Epoch [57/80], Step [200/500] Loss: 0.0026\n",
      "Epoch [57/80], Step [300/500] Loss: 0.0033\n",
      "Epoch [57/80], Step [400/500] Loss: 0.0011\n",
      "Epoch [57/80], Step [500/500] Loss: 0.0006\n",
      "Epoch [58/80], Step [100/500] Loss: 0.0015\n",
      "Epoch [58/80], Step [200/500] Loss: 0.0003\n",
      "Epoch [58/80], Step [300/500] Loss: 0.0107\n",
      "Epoch [58/80], Step [400/500] Loss: 0.0001\n",
      "Epoch [58/80], Step [500/500] Loss: 0.0078\n",
      "Epoch [59/80], Step [100/500] Loss: 0.0002\n",
      "Epoch [59/80], Step [200/500] Loss: 0.0001\n",
      "Epoch [59/80], Step [300/500] Loss: 0.0477\n",
      "Epoch [59/80], Step [400/500] Loss: 0.0013\n",
      "Epoch [59/80], Step [500/500] Loss: 0.0007\n",
      "Epoch [60/80], Step [100/500] Loss: 0.0005\n",
      "Epoch [60/80], Step [200/500] Loss: 0.0172\n",
      "Epoch [60/80], Step [300/500] Loss: 0.0015\n",
      "Epoch [60/80], Step [400/500] Loss: 0.0010\n",
      "Epoch [60/80], Step [500/500] Loss: 0.0030\n",
      "Epoch [61/80], Step [100/500] Loss: 0.0005\n",
      "Epoch [61/80], Step [200/500] Loss: 0.0016\n",
      "Epoch [61/80], Step [300/500] Loss: 0.0002\n",
      "Epoch [61/80], Step [400/500] Loss: 0.0002\n",
      "Epoch [61/80], Step [500/500] Loss: 0.0002\n",
      "Epoch [62/80], Step [100/500] Loss: 0.0015\n",
      "Epoch [62/80], Step [200/500] Loss: 0.0001\n",
      "Epoch [62/80], Step [300/500] Loss: 0.0004\n",
      "Epoch [62/80], Step [400/500] Loss: 0.0010\n",
      "Epoch [62/80], Step [500/500] Loss: 0.0006\n",
      "Epoch [63/80], Step [100/500] Loss: 0.0017\n",
      "Epoch [63/80], Step [200/500] Loss: 0.0004\n",
      "Epoch [63/80], Step [300/500] Loss: 0.0003\n",
      "Epoch [63/80], Step [400/500] Loss: 0.0004\n",
      "Epoch [63/80], Step [500/500] Loss: 0.0002\n",
      "Epoch [64/80], Step [100/500] Loss: 0.0096\n",
      "Epoch [64/80], Step [200/500] Loss: 0.0055\n",
      "Epoch [64/80], Step [300/500] Loss: 0.0110\n",
      "Epoch [64/80], Step [400/500] Loss: 0.0003\n",
      "Epoch [64/80], Step [500/500] Loss: 0.0001\n",
      "Epoch [65/80], Step [100/500] Loss: 0.0004\n",
      "Epoch [65/80], Step [200/500] Loss: 0.0000\n",
      "Epoch [65/80], Step [300/500] Loss: 0.0074\n",
      "Epoch [65/80], Step [400/500] Loss: 0.0002\n",
      "Epoch [65/80], Step [500/500] Loss: 0.0003\n",
      "Epoch [66/80], Step [100/500] Loss: 0.0005\n",
      "Epoch [66/80], Step [200/500] Loss: 0.0012\n",
      "Epoch [66/80], Step [300/500] Loss: 0.0008\n",
      "Epoch [66/80], Step [400/500] Loss: 0.0002\n",
      "Epoch [66/80], Step [500/500] Loss: 0.0005\n",
      "Epoch [67/80], Step [100/500] Loss: 0.0000\n",
      "Epoch [67/80], Step [200/500] Loss: 0.0076\n",
      "Epoch [67/80], Step [300/500] Loss: 0.0003\n",
      "Epoch [67/80], Step [400/500] Loss: 0.0012\n",
      "Epoch [67/80], Step [500/500] Loss: 0.0032\n",
      "Epoch [68/80], Step [100/500] Loss: 0.0001\n",
      "Epoch [68/80], Step [200/500] Loss: 0.0012\n",
      "Epoch [68/80], Step [300/500] Loss: 0.0036\n",
      "Epoch [68/80], Step [400/500] Loss: 0.0004\n",
      "Epoch [68/80], Step [500/500] Loss: 0.0022\n",
      "Epoch [69/80], Step [100/500] Loss: 0.0019\n",
      "Epoch [69/80], Step [200/500] Loss: 0.0000\n",
      "Epoch [69/80], Step [300/500] Loss: 0.0025\n",
      "Epoch [69/80], Step [400/500] Loss: 0.0001\n",
      "Epoch [69/80], Step [500/500] Loss: 0.0024\n",
      "Epoch [70/80], Step [100/500] Loss: 0.0011\n",
      "Epoch [70/80], Step [200/500] Loss: 0.0045\n",
      "Epoch [70/80], Step [300/500] Loss: 0.0000\n",
      "Epoch [70/80], Step [400/500] Loss: 0.0016\n",
      "Epoch [70/80], Step [500/500] Loss: 0.0046\n",
      "Epoch [71/80], Step [100/500] Loss: 0.0175\n",
      "Epoch [71/80], Step [200/500] Loss: 0.0016\n",
      "Epoch [71/80], Step [300/500] Loss: 0.0001\n",
      "Epoch [71/80], Step [400/500] Loss: 0.0001\n",
      "Epoch [71/80], Step [500/500] Loss: 0.0003\n",
      "Epoch [72/80], Step [100/500] Loss: 0.0018\n",
      "Epoch [72/80], Step [200/500] Loss: 0.0003\n",
      "Epoch [72/80], Step [300/500] Loss: 0.0004\n",
      "Epoch [72/80], Step [400/500] Loss: 0.0002\n",
      "Epoch [72/80], Step [500/500] Loss: 0.0000\n",
      "Epoch [73/80], Step [100/500] Loss: 0.0178\n",
      "Epoch [73/80], Step [200/500] Loss: 0.0001\n",
      "Epoch [73/80], Step [300/500] Loss: 0.0001\n",
      "Epoch [73/80], Step [400/500] Loss: 0.0001\n",
      "Epoch [73/80], Step [500/500] Loss: 0.0004\n",
      "Epoch [74/80], Step [100/500] Loss: 0.0001\n",
      "Epoch [74/80], Step [200/500] Loss: 0.0006\n",
      "Epoch [74/80], Step [300/500] Loss: 0.0041\n",
      "Epoch [74/80], Step [400/500] Loss: 0.0010\n",
      "Epoch [74/80], Step [500/500] Loss: 0.0015\n",
      "Epoch [75/80], Step [100/500] Loss: 0.0004\n",
      "Epoch [75/80], Step [200/500] Loss: 0.0282\n",
      "Epoch [75/80], Step [300/500] Loss: 0.0003\n",
      "Epoch [75/80], Step [400/500] Loss: 0.0003\n",
      "Epoch [75/80], Step [500/500] Loss: 0.0012\n",
      "Epoch [76/80], Step [100/500] Loss: 0.0003\n",
      "Epoch [76/80], Step [200/500] Loss: 0.0022\n",
      "Epoch [76/80], Step [300/500] Loss: 0.0002\n",
      "Epoch [76/80], Step [400/500] Loss: 0.0000\n",
      "Epoch [76/80], Step [500/500] Loss: 0.0004\n",
      "Epoch [77/80], Step [100/500] Loss: 0.0001\n",
      "Epoch [77/80], Step [200/500] Loss: 0.0001\n",
      "Epoch [77/80], Step [300/500] Loss: 0.0000\n",
      "Epoch [77/80], Step [400/500] Loss: 0.0002\n",
      "Epoch [77/80], Step [500/500] Loss: 0.0006\n",
      "Epoch [78/80], Step [100/500] Loss: 0.0025\n",
      "Epoch [78/80], Step [200/500] Loss: 0.0064\n",
      "Epoch [78/80], Step [300/500] Loss: 0.0007\n",
      "Epoch [78/80], Step [400/500] Loss: 0.0009\n",
      "Epoch [78/80], Step [500/500] Loss: 0.0000\n",
      "Epoch [79/80], Step [100/500] Loss: 0.0009\n",
      "Epoch [79/80], Step [200/500] Loss: 0.0001\n",
      "Epoch [79/80], Step [300/500] Loss: 0.0013\n",
      "Epoch [79/80], Step [400/500] Loss: 0.0000\n",
      "Epoch [79/80], Step [500/500] Loss: 0.0039\n",
      "Epoch [80/80], Step [100/500] Loss: 0.0000\n",
      "Epoch [80/80], Step [200/500] Loss: 0.0012\n",
      "Epoch [80/80], Step [300/500] Loss: 0.0001\n",
      "Epoch [80/80], Step [400/500] Loss: 0.0001\n",
      "Epoch [80/80], Step [500/500] Loss: 0.0011\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "curr_lr = learning_rate\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    # Decay learning rate\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y6bcORU2sTFG",
    "outputId": "ccc26606-1ea8-4e47-f4d3-b60d1625b5e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 92.99 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mON8t8P_tP4F"
   },
   "outputs": [],
   "source": [
    "model = ResNet(Bottleneck, [3, 4, 6, 3]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k7NZ_Iv0tRkW"
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "num_epochs = 80\n",
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z92G3erttTVN"
   },
   "outputs": [],
   "source": [
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ac4L5MtXtVV1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Step [100/500] Loss: 1.8722\n",
      "Epoch [1/80], Step [200/500] Loss: 1.5546\n",
      "Epoch [1/80], Step [300/500] Loss: 1.4912\n",
      "Epoch [1/80], Step [400/500] Loss: 1.3069\n",
      "Epoch [1/80], Step [500/500] Loss: 1.1303\n",
      "Epoch [2/80], Step [100/500] Loss: 1.1997\n",
      "Epoch [2/80], Step [200/500] Loss: 1.3460\n",
      "Epoch [2/80], Step [300/500] Loss: 1.1125\n",
      "Epoch [2/80], Step [400/500] Loss: 0.9988\n",
      "Epoch [2/80], Step [500/500] Loss: 0.9444\n",
      "Epoch [3/80], Step [100/500] Loss: 0.8973\n",
      "Epoch [3/80], Step [200/500] Loss: 1.0010\n",
      "Epoch [3/80], Step [300/500] Loss: 1.0233\n",
      "Epoch [3/80], Step [400/500] Loss: 0.7729\n",
      "Epoch [3/80], Step [500/500] Loss: 0.8495\n",
      "Epoch [4/80], Step [100/500] Loss: 0.8925\n",
      "Epoch [4/80], Step [200/500] Loss: 0.8603\n",
      "Epoch [4/80], Step [300/500] Loss: 0.6710\n",
      "Epoch [4/80], Step [400/500] Loss: 0.6513\n",
      "Epoch [4/80], Step [500/500] Loss: 0.5553\n",
      "Epoch [5/80], Step [100/500] Loss: 0.6803\n",
      "Epoch [5/80], Step [200/500] Loss: 0.7425\n",
      "Epoch [5/80], Step [300/500] Loss: 0.6266\n",
      "Epoch [5/80], Step [400/500] Loss: 0.5237\n",
      "Epoch [5/80], Step [500/500] Loss: 0.6167\n",
      "Epoch [6/80], Step [100/500] Loss: 0.6040\n",
      "Epoch [6/80], Step [200/500] Loss: 0.6839\n",
      "Epoch [6/80], Step [300/500] Loss: 0.5336\n",
      "Epoch [6/80], Step [400/500] Loss: 0.4267\n",
      "Epoch [6/80], Step [500/500] Loss: 0.6117\n",
      "Epoch [7/80], Step [100/500] Loss: 0.6107\n",
      "Epoch [7/80], Step [200/500] Loss: 0.4913\n",
      "Epoch [7/80], Step [300/500] Loss: 0.4799\n",
      "Epoch [7/80], Step [400/500] Loss: 0.4237\n",
      "Epoch [7/80], Step [500/500] Loss: 0.5870\n",
      "Epoch [8/80], Step [100/500] Loss: 0.5305\n",
      "Epoch [8/80], Step [200/500] Loss: 0.3510\n",
      "Epoch [8/80], Step [300/500] Loss: 0.4988\n",
      "Epoch [8/80], Step [400/500] Loss: 0.4568\n",
      "Epoch [8/80], Step [500/500] Loss: 0.3563\n",
      "Epoch [9/80], Step [100/500] Loss: 0.3312\n",
      "Epoch [9/80], Step [200/500] Loss: 0.5787\n",
      "Epoch [9/80], Step [300/500] Loss: 0.4894\n",
      "Epoch [9/80], Step [400/500] Loss: 0.5592\n",
      "Epoch [9/80], Step [500/500] Loss: 0.3931\n",
      "Epoch [10/80], Step [100/500] Loss: 0.2479\n",
      "Epoch [10/80], Step [200/500] Loss: 0.5334\n",
      "Epoch [10/80], Step [300/500] Loss: 0.4878\n",
      "Epoch [10/80], Step [400/500] Loss: 0.3024\n",
      "Epoch [10/80], Step [500/500] Loss: 0.3595\n",
      "Epoch [11/80], Step [100/500] Loss: 0.3308\n",
      "Epoch [11/80], Step [200/500] Loss: 0.3361\n",
      "Epoch [11/80], Step [300/500] Loss: 0.4528\n",
      "Epoch [11/80], Step [400/500] Loss: 0.4369\n",
      "Epoch [11/80], Step [500/500] Loss: 0.3694\n",
      "Epoch [12/80], Step [100/500] Loss: 0.3149\n",
      "Epoch [12/80], Step [200/500] Loss: 0.3146\n",
      "Epoch [12/80], Step [300/500] Loss: 0.4131\n",
      "Epoch [12/80], Step [400/500] Loss: 0.3852\n",
      "Epoch [12/80], Step [500/500] Loss: 0.3262\n",
      "Epoch [13/80], Step [100/500] Loss: 0.5749\n",
      "Epoch [13/80], Step [200/500] Loss: 0.4387\n",
      "Epoch [13/80], Step [300/500] Loss: 0.3069\n",
      "Epoch [13/80], Step [400/500] Loss: 0.4907\n",
      "Epoch [13/80], Step [500/500] Loss: 0.3449\n",
      "Epoch [14/80], Step [100/500] Loss: 0.4766\n",
      "Epoch [14/80], Step [200/500] Loss: 0.4844\n",
      "Epoch [14/80], Step [300/500] Loss: 0.3352\n",
      "Epoch [14/80], Step [400/500] Loss: 0.3094\n",
      "Epoch [14/80], Step [500/500] Loss: 0.3121\n",
      "Epoch [15/80], Step [100/500] Loss: 0.2042\n",
      "Epoch [15/80], Step [200/500] Loss: 0.3181\n",
      "Epoch [15/80], Step [300/500] Loss: 0.2946\n",
      "Epoch [15/80], Step [400/500] Loss: 0.3979\n",
      "Epoch [15/80], Step [500/500] Loss: 0.2253\n",
      "Epoch [16/80], Step [100/500] Loss: 0.2254\n",
      "Epoch [16/80], Step [200/500] Loss: 0.2254\n",
      "Epoch [16/80], Step [300/500] Loss: 0.3798\n",
      "Epoch [16/80], Step [400/500] Loss: 0.3117\n",
      "Epoch [16/80], Step [500/500] Loss: 0.2867\n",
      "Epoch [17/80], Step [100/500] Loss: 0.1973\n",
      "Epoch [17/80], Step [200/500] Loss: 0.3230\n",
      "Epoch [17/80], Step [300/500] Loss: 0.2243\n",
      "Epoch [17/80], Step [400/500] Loss: 0.2503\n",
      "Epoch [17/80], Step [500/500] Loss: 0.2332\n",
      "Epoch [18/80], Step [100/500] Loss: 0.4282\n",
      "Epoch [18/80], Step [200/500] Loss: 0.2111\n",
      "Epoch [18/80], Step [300/500] Loss: 0.2676\n",
      "Epoch [18/80], Step [400/500] Loss: 0.3435\n",
      "Epoch [18/80], Step [500/500] Loss: 0.2659\n",
      "Epoch [19/80], Step [100/500] Loss: 0.3698\n",
      "Epoch [19/80], Step [200/500] Loss: 0.1811\n",
      "Epoch [19/80], Step [300/500] Loss: 0.2976\n",
      "Epoch [19/80], Step [400/500] Loss: 0.4254\n",
      "Epoch [19/80], Step [500/500] Loss: 0.2882\n",
      "Epoch [20/80], Step [100/500] Loss: 0.2791\n",
      "Epoch [20/80], Step [200/500] Loss: 0.3137\n",
      "Epoch [20/80], Step [300/500] Loss: 0.2722\n",
      "Epoch [20/80], Step [400/500] Loss: 0.1602\n",
      "Epoch [20/80], Step [500/500] Loss: 0.2515\n",
      "Epoch [21/80], Step [100/500] Loss: 0.1579\n",
      "Epoch [21/80], Step [200/500] Loss: 0.3435\n",
      "Epoch [21/80], Step [300/500] Loss: 0.2059\n",
      "Epoch [21/80], Step [400/500] Loss: 0.0564\n",
      "Epoch [21/80], Step [500/500] Loss: 0.1409\n",
      "Epoch [22/80], Step [100/500] Loss: 0.1576\n",
      "Epoch [22/80], Step [200/500] Loss: 0.1394\n",
      "Epoch [22/80], Step [300/500] Loss: 0.1725\n",
      "Epoch [22/80], Step [400/500] Loss: 0.1306\n",
      "Epoch [22/80], Step [500/500] Loss: 0.1605\n",
      "Epoch [23/80], Step [100/500] Loss: 0.1097\n",
      "Epoch [23/80], Step [200/500] Loss: 0.0553\n",
      "Epoch [23/80], Step [300/500] Loss: 0.1483\n",
      "Epoch [23/80], Step [400/500] Loss: 0.1761\n",
      "Epoch [23/80], Step [500/500] Loss: 0.0757\n",
      "Epoch [24/80], Step [100/500] Loss: 0.0835\n",
      "Epoch [24/80], Step [200/500] Loss: 0.1112\n",
      "Epoch [24/80], Step [300/500] Loss: 0.0783\n",
      "Epoch [24/80], Step [400/500] Loss: 0.1411\n",
      "Epoch [24/80], Step [500/500] Loss: 0.0885\n",
      "Epoch [25/80], Step [100/500] Loss: 0.0475\n",
      "Epoch [25/80], Step [200/500] Loss: 0.0508\n",
      "Epoch [25/80], Step [300/500] Loss: 0.1316\n",
      "Epoch [25/80], Step [400/500] Loss: 0.1119\n",
      "Epoch [25/80], Step [500/500] Loss: 0.0317\n",
      "Epoch [26/80], Step [100/500] Loss: 0.0339\n",
      "Epoch [26/80], Step [200/500] Loss: 0.1842\n",
      "Epoch [26/80], Step [300/500] Loss: 0.0527\n",
      "Epoch [26/80], Step [400/500] Loss: 0.1371\n",
      "Epoch [26/80], Step [500/500] Loss: 0.0346\n",
      "Epoch [27/80], Step [100/500] Loss: 0.0662\n",
      "Epoch [27/80], Step [200/500] Loss: 0.1099\n",
      "Epoch [27/80], Step [300/500] Loss: 0.0957\n",
      "Epoch [27/80], Step [400/500] Loss: 0.1050\n",
      "Epoch [27/80], Step [500/500] Loss: 0.0714\n",
      "Epoch [28/80], Step [100/500] Loss: 0.0779\n",
      "Epoch [28/80], Step [200/500] Loss: 0.1013\n",
      "Epoch [28/80], Step [300/500] Loss: 0.1834\n",
      "Epoch [28/80], Step [400/500] Loss: 0.1322\n",
      "Epoch [28/80], Step [500/500] Loss: 0.1250\n",
      "Epoch [29/80], Step [100/500] Loss: 0.0654\n",
      "Epoch [29/80], Step [200/500] Loss: 0.0416\n",
      "Epoch [29/80], Step [300/500] Loss: 0.1011\n",
      "Epoch [29/80], Step [400/500] Loss: 0.0398\n",
      "Epoch [29/80], Step [500/500] Loss: 0.0949\n",
      "Epoch [30/80], Step [100/500] Loss: 0.0522\n",
      "Epoch [30/80], Step [200/500] Loss: 0.1069\n",
      "Epoch [30/80], Step [300/500] Loss: 0.0560\n",
      "Epoch [30/80], Step [400/500] Loss: 0.0489\n",
      "Epoch [30/80], Step [500/500] Loss: 0.0488\n",
      "Epoch [31/80], Step [100/500] Loss: 0.0884\n",
      "Epoch [31/80], Step [200/500] Loss: 0.0674\n",
      "Epoch [31/80], Step [300/500] Loss: 0.0360\n",
      "Epoch [31/80], Step [400/500] Loss: 0.0582\n",
      "Epoch [31/80], Step [500/500] Loss: 0.0388\n",
      "Epoch [32/80], Step [100/500] Loss: 0.0431\n",
      "Epoch [32/80], Step [200/500] Loss: 0.0678\n",
      "Epoch [32/80], Step [300/500] Loss: 0.0524\n",
      "Epoch [32/80], Step [400/500] Loss: 0.0699\n",
      "Epoch [32/80], Step [500/500] Loss: 0.0308\n",
      "Epoch [33/80], Step [100/500] Loss: 0.0394\n",
      "Epoch [33/80], Step [200/500] Loss: 0.0584\n",
      "Epoch [33/80], Step [300/500] Loss: 0.0336\n",
      "Epoch [33/80], Step [400/500] Loss: 0.0179\n",
      "Epoch [33/80], Step [500/500] Loss: 0.0910\n",
      "Epoch [34/80], Step [100/500] Loss: 0.0452\n",
      "Epoch [34/80], Step [200/500] Loss: 0.0315\n",
      "Epoch [34/80], Step [300/500] Loss: 0.0542\n",
      "Epoch [34/80], Step [400/500] Loss: 0.0260\n",
      "Epoch [34/80], Step [500/500] Loss: 0.0379\n",
      "Epoch [35/80], Step [100/500] Loss: 0.0303\n",
      "Epoch [35/80], Step [200/500] Loss: 0.0077\n",
      "Epoch [35/80], Step [300/500] Loss: 0.0578\n",
      "Epoch [35/80], Step [400/500] Loss: 0.0956\n",
      "Epoch [35/80], Step [500/500] Loss: 0.0517\n",
      "Epoch [36/80], Step [100/500] Loss: 0.0164\n",
      "Epoch [36/80], Step [200/500] Loss: 0.0350\n",
      "Epoch [36/80], Step [300/500] Loss: 0.0605\n",
      "Epoch [36/80], Step [400/500] Loss: 0.2324\n",
      "Epoch [36/80], Step [500/500] Loss: 0.0627\n",
      "Epoch [37/80], Step [100/500] Loss: 0.0257\n",
      "Epoch [37/80], Step [200/500] Loss: 0.0403\n",
      "Epoch [37/80], Step [300/500] Loss: 0.0805\n",
      "Epoch [37/80], Step [400/500] Loss: 0.0694\n",
      "Epoch [37/80], Step [500/500] Loss: 0.0438\n",
      "Epoch [38/80], Step [100/500] Loss: 0.0391\n",
      "Epoch [38/80], Step [200/500] Loss: 0.0415\n",
      "Epoch [38/80], Step [300/500] Loss: 0.0910\n",
      "Epoch [38/80], Step [400/500] Loss: 0.1116\n",
      "Epoch [38/80], Step [500/500] Loss: 0.0276\n",
      "Epoch [39/80], Step [100/500] Loss: 0.0466\n",
      "Epoch [39/80], Step [200/500] Loss: 0.0214\n",
      "Epoch [39/80], Step [300/500] Loss: 0.0931\n",
      "Epoch [39/80], Step [400/500] Loss: 0.0871\n",
      "Epoch [39/80], Step [500/500] Loss: 0.0389\n",
      "Epoch [40/80], Step [100/500] Loss: 0.0095\n",
      "Epoch [40/80], Step [200/500] Loss: 0.0427\n",
      "Epoch [40/80], Step [300/500] Loss: 0.0330\n",
      "Epoch [40/80], Step [400/500] Loss: 0.0216\n",
      "Epoch [40/80], Step [500/500] Loss: 0.0266\n",
      "Epoch [41/80], Step [100/500] Loss: 0.0383\n",
      "Epoch [41/80], Step [200/500] Loss: 0.0482\n",
      "Epoch [41/80], Step [300/500] Loss: 0.0186\n",
      "Epoch [41/80], Step [400/500] Loss: 0.0287\n",
      "Epoch [41/80], Step [500/500] Loss: 0.0823\n",
      "Epoch [42/80], Step [100/500] Loss: 0.0375\n",
      "Epoch [42/80], Step [200/500] Loss: 0.0144\n",
      "Epoch [42/80], Step [300/500] Loss: 0.0029\n",
      "Epoch [42/80], Step [400/500] Loss: 0.0036\n",
      "Epoch [42/80], Step [500/500] Loss: 0.0016\n",
      "Epoch [43/80], Step [100/500] Loss: 0.0105\n",
      "Epoch [43/80], Step [200/500] Loss: 0.0300\n",
      "Epoch [43/80], Step [300/500] Loss: 0.0440\n",
      "Epoch [43/80], Step [400/500] Loss: 0.0425\n",
      "Epoch [43/80], Step [500/500] Loss: 0.0027\n",
      "Epoch [44/80], Step [100/500] Loss: 0.0055\n",
      "Epoch [44/80], Step [200/500] Loss: 0.0064\n",
      "Epoch [44/80], Step [300/500] Loss: 0.0306\n",
      "Epoch [44/80], Step [400/500] Loss: 0.0389\n",
      "Epoch [44/80], Step [500/500] Loss: 0.0061\n",
      "Epoch [45/80], Step [100/500] Loss: 0.0071\n",
      "Epoch [45/80], Step [200/500] Loss: 0.0018\n",
      "Epoch [45/80], Step [300/500] Loss: 0.0019\n",
      "Epoch [45/80], Step [400/500] Loss: 0.0046\n",
      "Epoch [45/80], Step [500/500] Loss: 0.0022\n",
      "Epoch [46/80], Step [100/500] Loss: 0.0048\n",
      "Epoch [46/80], Step [200/500] Loss: 0.0314\n",
      "Epoch [46/80], Step [300/500] Loss: 0.0047\n",
      "Epoch [46/80], Step [400/500] Loss: 0.0482\n",
      "Epoch [46/80], Step [500/500] Loss: 0.0105\n",
      "Epoch [47/80], Step [100/500] Loss: 0.0161\n",
      "Epoch [47/80], Step [200/500] Loss: 0.0080\n",
      "Epoch [47/80], Step [300/500] Loss: 0.0135\n",
      "Epoch [47/80], Step [400/500] Loss: 0.0103\n",
      "Epoch [47/80], Step [500/500] Loss: 0.0344\n",
      "Epoch [48/80], Step [100/500] Loss: 0.0073\n",
      "Epoch [48/80], Step [200/500] Loss: 0.0016\n",
      "Epoch [48/80], Step [300/500] Loss: 0.0005\n",
      "Epoch [48/80], Step [400/500] Loss: 0.0041\n",
      "Epoch [48/80], Step [500/500] Loss: 0.0024\n",
      "Epoch [49/80], Step [100/500] Loss: 0.0488\n",
      "Epoch [49/80], Step [200/500] Loss: 0.0056\n",
      "Epoch [49/80], Step [300/500] Loss: 0.0041\n",
      "Epoch [49/80], Step [400/500] Loss: 0.0055\n",
      "Epoch [49/80], Step [500/500] Loss: 0.0152\n",
      "Epoch [50/80], Step [100/500] Loss: 0.0344\n",
      "Epoch [50/80], Step [200/500] Loss: 0.0160\n",
      "Epoch [50/80], Step [300/500] Loss: 0.0029\n",
      "Epoch [50/80], Step [400/500] Loss: 0.0020\n",
      "Epoch [50/80], Step [500/500] Loss: 0.0117\n",
      "Epoch [51/80], Step [100/500] Loss: 0.0002\n",
      "Epoch [51/80], Step [200/500] Loss: 0.0005\n",
      "Epoch [51/80], Step [300/500] Loss: 0.0108\n",
      "Epoch [51/80], Step [400/500] Loss: 0.0104\n",
      "Epoch [51/80], Step [500/500] Loss: 0.0004\n",
      "Epoch [52/80], Step [100/500] Loss: 0.0006\n",
      "Epoch [52/80], Step [200/500] Loss: 0.0149\n",
      "Epoch [52/80], Step [300/500] Loss: 0.0008\n",
      "Epoch [52/80], Step [400/500] Loss: 0.0108\n",
      "Epoch [52/80], Step [500/500] Loss: 0.0007\n",
      "Epoch [53/80], Step [100/500] Loss: 0.0120\n",
      "Epoch [53/80], Step [200/500] Loss: 0.0029\n",
      "Epoch [53/80], Step [300/500] Loss: 0.0944\n",
      "Epoch [53/80], Step [400/500] Loss: 0.0047\n",
      "Epoch [53/80], Step [500/500] Loss: 0.0127\n",
      "Epoch [54/80], Step [100/500] Loss: 0.0014\n",
      "Epoch [54/80], Step [200/500] Loss: 0.0012\n",
      "Epoch [54/80], Step [300/500] Loss: 0.0299\n",
      "Epoch [54/80], Step [400/500] Loss: 0.0124\n",
      "Epoch [54/80], Step [500/500] Loss: 0.0055\n",
      "Epoch [55/80], Step [100/500] Loss: 0.0045\n",
      "Epoch [55/80], Step [200/500] Loss: 0.0019\n",
      "Epoch [55/80], Step [300/500] Loss: 0.0168\n",
      "Epoch [55/80], Step [400/500] Loss: 0.0012\n",
      "Epoch [55/80], Step [500/500] Loss: 0.0068\n",
      "Epoch [56/80], Step [100/500] Loss: 0.0025\n",
      "Epoch [56/80], Step [200/500] Loss: 0.0037\n",
      "Epoch [56/80], Step [300/500] Loss: 0.0267\n",
      "Epoch [56/80], Step [400/500] Loss: 0.0001\n",
      "Epoch [56/80], Step [500/500] Loss: 0.0012\n",
      "Epoch [57/80], Step [100/500] Loss: 0.0007\n",
      "Epoch [57/80], Step [200/500] Loss: 0.0088\n",
      "Epoch [57/80], Step [300/500] Loss: 0.0108\n",
      "Epoch [57/80], Step [400/500] Loss: 0.0431\n",
      "Epoch [57/80], Step [500/500] Loss: 0.0291\n",
      "Epoch [58/80], Step [100/500] Loss: 0.0014\n",
      "Epoch [58/80], Step [200/500] Loss: 0.0040\n",
      "Epoch [58/80], Step [300/500] Loss: 0.0207\n",
      "Epoch [58/80], Step [400/500] Loss: 0.0025\n",
      "Epoch [58/80], Step [500/500] Loss: 0.0005\n",
      "Epoch [59/80], Step [100/500] Loss: 0.0076\n",
      "Epoch [59/80], Step [200/500] Loss: 0.0005\n",
      "Epoch [59/80], Step [300/500] Loss: 0.0482\n",
      "Epoch [59/80], Step [400/500] Loss: 0.0212\n",
      "Epoch [59/80], Step [500/500] Loss: 0.0036\n",
      "Epoch [60/80], Step [100/500] Loss: 0.0004\n",
      "Epoch [60/80], Step [200/500] Loss: 0.0137\n",
      "Epoch [60/80], Step [300/500] Loss: 0.0044\n",
      "Epoch [60/80], Step [400/500] Loss: 0.0002\n",
      "Epoch [60/80], Step [500/500] Loss: 0.0005\n",
      "Epoch [61/80], Step [100/500] Loss: 0.0263\n",
      "Epoch [61/80], Step [200/500] Loss: 0.0005\n",
      "Epoch [61/80], Step [300/500] Loss: 0.0078\n",
      "Epoch [61/80], Step [400/500] Loss: 0.0047\n",
      "Epoch [61/80], Step [500/500] Loss: 0.0036\n",
      "Epoch [62/80], Step [100/500] Loss: 0.0040\n",
      "Epoch [62/80], Step [200/500] Loss: 0.0211\n",
      "Epoch [62/80], Step [300/500] Loss: 0.0007\n",
      "Epoch [62/80], Step [400/500] Loss: 0.0030\n",
      "Epoch [62/80], Step [500/500] Loss: 0.0013\n",
      "Epoch [63/80], Step [100/500] Loss: 0.0012\n",
      "Epoch [63/80], Step [200/500] Loss: 0.0013\n",
      "Epoch [63/80], Step [300/500] Loss: 0.0182\n",
      "Epoch [63/80], Step [400/500] Loss: 0.0001\n",
      "Epoch [63/80], Step [500/500] Loss: 0.0009\n",
      "Epoch [64/80], Step [100/500] Loss: 0.0004\n",
      "Epoch [64/80], Step [200/500] Loss: 0.0004\n",
      "Epoch [64/80], Step [300/500] Loss: 0.0007\n",
      "Epoch [64/80], Step [400/500] Loss: 0.0001\n",
      "Epoch [64/80], Step [500/500] Loss: 0.0004\n",
      "Epoch [65/80], Step [100/500] Loss: 0.0014\n",
      "Epoch [65/80], Step [200/500] Loss: 0.0004\n",
      "Epoch [65/80], Step [300/500] Loss: 0.0004\n",
      "Epoch [65/80], Step [400/500] Loss: 0.0006\n",
      "Epoch [65/80], Step [500/500] Loss: 0.0011\n",
      "Epoch [66/80], Step [100/500] Loss: 0.0017\n",
      "Epoch [66/80], Step [200/500] Loss: 0.0473\n",
      "Epoch [66/80], Step [300/500] Loss: 0.0004\n",
      "Epoch [66/80], Step [400/500] Loss: 0.0010\n",
      "Epoch [66/80], Step [500/500] Loss: 0.0004\n",
      "Epoch [67/80], Step [100/500] Loss: 0.0118\n",
      "Epoch [67/80], Step [200/500] Loss: 0.0006\n",
      "Epoch [67/80], Step [300/500] Loss: 0.0004\n",
      "Epoch [67/80], Step [400/500] Loss: 0.0005\n",
      "Epoch [67/80], Step [500/500] Loss: 0.0058\n",
      "Epoch [68/80], Step [100/500] Loss: 0.0002\n",
      "Epoch [68/80], Step [200/500] Loss: 0.0003\n",
      "Epoch [68/80], Step [300/500] Loss: 0.0001\n",
      "Epoch [68/80], Step [400/500] Loss: 0.0047\n",
      "Epoch [68/80], Step [500/500] Loss: 0.0385\n",
      "Epoch [69/80], Step [100/500] Loss: 0.0004\n",
      "Epoch [69/80], Step [200/500] Loss: 0.0001\n",
      "Epoch [69/80], Step [300/500] Loss: 0.0013\n",
      "Epoch [69/80], Step [400/500] Loss: 0.0000\n",
      "Epoch [69/80], Step [500/500] Loss: 0.0132\n",
      "Epoch [70/80], Step [100/500] Loss: 0.0011\n",
      "Epoch [70/80], Step [200/500] Loss: 0.0002\n",
      "Epoch [70/80], Step [300/500] Loss: 0.0004\n",
      "Epoch [70/80], Step [400/500] Loss: 0.0015\n",
      "Epoch [70/80], Step [500/500] Loss: 0.0011\n",
      "Epoch [71/80], Step [100/500] Loss: 0.0007\n",
      "Epoch [71/80], Step [200/500] Loss: 0.0006\n",
      "Epoch [71/80], Step [300/500] Loss: 0.0000\n",
      "Epoch [71/80], Step [400/500] Loss: 0.0004\n",
      "Epoch [71/80], Step [500/500] Loss: 0.0011\n",
      "Epoch [72/80], Step [100/500] Loss: 0.0020\n",
      "Epoch [72/80], Step [200/500] Loss: 0.0005\n",
      "Epoch [72/80], Step [300/500] Loss: 0.0001\n",
      "Epoch [72/80], Step [400/500] Loss: 0.0008\n",
      "Epoch [72/80], Step [500/500] Loss: 0.0009\n",
      "Epoch [73/80], Step [100/500] Loss: 0.0002\n",
      "Epoch [73/80], Step [200/500] Loss: 0.0007\n",
      "Epoch [73/80], Step [300/500] Loss: 0.0003\n",
      "Epoch [73/80], Step [400/500] Loss: 0.0019\n",
      "Epoch [73/80], Step [500/500] Loss: 0.0535\n",
      "Epoch [74/80], Step [100/500] Loss: 0.0004\n",
      "Epoch [74/80], Step [200/500] Loss: 0.0009\n",
      "Epoch [74/80], Step [300/500] Loss: 0.0111\n",
      "Epoch [74/80], Step [400/500] Loss: 0.0020\n",
      "Epoch [74/80], Step [500/500] Loss: 0.0006\n",
      "Epoch [75/80], Step [100/500] Loss: 0.0005\n",
      "Epoch [75/80], Step [200/500] Loss: 0.0002\n",
      "Epoch [75/80], Step [300/500] Loss: 0.0031\n",
      "Epoch [75/80], Step [400/500] Loss: 0.0004\n",
      "Epoch [75/80], Step [500/500] Loss: 0.0016\n",
      "Epoch [76/80], Step [100/500] Loss: 0.0002\n",
      "Epoch [76/80], Step [200/500] Loss: 0.0004\n",
      "Epoch [76/80], Step [300/500] Loss: 0.0091\n",
      "Epoch [76/80], Step [400/500] Loss: 0.0004\n",
      "Epoch [76/80], Step [500/500] Loss: 0.0051\n",
      "Epoch [77/80], Step [100/500] Loss: 0.0005\n",
      "Epoch [77/80], Step [200/500] Loss: 0.0002\n",
      "Epoch [77/80], Step [300/500] Loss: 0.0055\n",
      "Epoch [77/80], Step [400/500] Loss: 0.0004\n",
      "Epoch [77/80], Step [500/500] Loss: 0.0002\n",
      "Epoch [78/80], Step [100/500] Loss: 0.0004\n",
      "Epoch [78/80], Step [200/500] Loss: 0.0008\n",
      "Epoch [78/80], Step [300/500] Loss: 0.0005\n",
      "Epoch [78/80], Step [400/500] Loss: 0.0075\n",
      "Epoch [78/80], Step [500/500] Loss: 0.0008\n",
      "Epoch [79/80], Step [100/500] Loss: 0.0012\n",
      "Epoch [79/80], Step [200/500] Loss: 0.0001\n",
      "Epoch [79/80], Step [300/500] Loss: 0.0003\n",
      "Epoch [79/80], Step [400/500] Loss: 0.0006\n",
      "Epoch [79/80], Step [500/500] Loss: 0.0313\n",
      "Epoch [80/80], Step [100/500] Loss: 0.0009\n",
      "Epoch [80/80], Step [200/500] Loss: 0.0010\n",
      "Epoch [80/80], Step [300/500] Loss: 0.0007\n",
      "Epoch [80/80], Step [400/500] Loss: 0.0003\n",
      "Epoch [80/80], Step [500/500] Loss: 0.0031\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "curr_lr = learning_rate\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    # Decay learning rate\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s56CX27CtXKM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 92.76 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ResNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0b27470a2ac345b185d697f3189ffe7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1969bb5949b14a57b738edca76e7e491": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e2aa9dbcaaa1451992b9bc4c03d7c0f3",
       "IPY_MODEL_30956606c84a4aab97dc1e319aa0a95f"
      ],
      "layout": "IPY_MODEL_7a4405dc27a746f88d517895cceb4e2c"
     }
    },
    "23832a2ff17346ee968ae64edef05432": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30956606c84a4aab97dc1e319aa0a95f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23832a2ff17346ee968ae64edef05432",
      "placeholder": "​",
      "style": "IPY_MODEL_0b27470a2ac345b185d697f3189ffe7f",
      "value": " 170500096/? [00:20&lt;00:00, 52938211.96it/s]"
     }
    },
    "7a4405dc27a746f88d517895cceb4e2c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7055e1d40b843dda95121da9dc72cea": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2aa9dbcaaa1451992b9bc4c03d7c0f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7055e1d40b843dda95121da9dc72cea",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fc2fe569dede45789a518eaa503ecc85",
      "value": 1
     }
    },
    "fc2fe569dede45789a518eaa503ecc85": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
